{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the SSVEP data\n",
    "\n",
    "Let's load a couple libraries we'll use repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(dplyr, warn.conflicts=FALSE)\n",
    "library(ggplot2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe we're about to load has already undergone fourier transform & projection to FSAverage cortical space, so we're dealing with vertex numbers and frequency bin amplitudes, across subject and pre/post intervention measurement times.  This assumes it's in the same folder as the notebook; adjust path as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsed with column specification:\n",
      "cols(\n",
      "  subject = \u001b[31mcol_character()\u001b[39m,\n",
      "  freq = \u001b[32mcol_double()\u001b[39m,\n",
      "  source = \u001b[31mcol_character()\u001b[39m,\n",
      "  value = \u001b[32mcol_double()\u001b[39m,\n",
      "  timepoint = \u001b[31mcol_character()\u001b[39m\n",
      ")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user  system elapsed \n",
      "173.321  17.933 191.369 \n"
     ]
    }
   ],
   "source": [
    "proc.time() -> start\n",
    "readr::read_csv(\"all_subjects-fsaverage-freq_domain-stc.csv\") -> all_data\n",
    "print(proc.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add in some metadata about who is in which group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"https://raw.githubusercontent.com/yeatmanlab/SSWEF/master/params\" -> param_dir\n",
    "file.path(\"..\", \"..\", \"params\") -> param_dir\n",
    "\n",
    "# load intervention groups\n",
    "file.path(param_dir, \"intervention_cohorts.yaml\") -> intervention_file\n",
    "yaml::read_yaml(intervention_file) -> intervention\n",
    "\n",
    "# load pre-test cohorts\n",
    "file.path(param_dir, \"letter_knowledge_cohorts.yaml\") -> pretest_file\n",
    "yaml::read_yaml(pretest_file) -> pretest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll define a function that will compute noise (the mean of the 2 frequency bins below and 2 frequency bins above the current bin). We'll apply it within a `group_by` operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute noise (mean of 2 bins above and below)\n",
    "compute_noise <- function(x) {\n",
    "    (lag(x, 2, 0) + lag(x, 1, 0) + lead(x, 1, 0) + lead(x, 2, 0)) / 4 -> noise\n",
    "    return(noise)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll merge in the metadata, separate the `source` column into separate hemisphere and vertex number columns, and compute `noise` and `snr` all in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes several minutes... might be worth it to split it into steps that assign back to `all_data`\n",
    "# the comment lines are natural points at which to do this.\n",
    "all_data %>%\n",
    "    # merge in metadata\n",
    "    mutate(subj_num=stringr::str_sub(subject, -4),\n",
    "           pretest=ifelse(subj_num %in% pretest$LowerKnowledge, 'lower', 'upper'),\n",
    "           intervention=ifelse(subj_num %in% intervention$LetterIntervention, 'letter', 'language')) %>%\n",
    "    # separate the hemispheres\n",
    "    tidyr::separate(source, c(\"hemi\", \"vertex\"), sep=\"_\") %>%\n",
    "    mutate(hemi=stringr::str_to_lower(hemi),\n",
    "           vertex=as.integer(vertex),\n",
    "           source=NULL) %>%\n",
    "    # compute noise and SNR\n",
    "    group_by(subject, timepoint, hemi, vertex) %>%\n",
    "    mutate(noise=compute_noise(value),\n",
    "           snr=value/noise) %>%\n",
    "    ungroup() ->\n",
    "    all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save ourselves the trouble of having to do that again, by saving the processed data, so we can skip ahead next time we run this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(all_data, file=\"processed_data.RData\")  # binary format, more efficient\n",
    "# load(\"processed_data.RData\")\n",
    "\n",
    "# readr::write_csv(all_data, \"processed_data.csv\")\n",
    "# readr::read_csv(\"processed_data.csv\") -> all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dataframe has all the variables we want, let's compute our (uncorrected) t-test of \"signal bin\" vs \"mean of adjacent noise bins\". This will yield a new dataframe with the t-test results. We'll write out the t-value table too, so we don't need to recompute it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute signal-vs-noise uncorrected t-test\n",
    "all_data %>%\n",
    "    filter(freq == 2, timepoint == \"pre\") %>%\n",
    "    group_by(hemi, vertex) %>%\n",
    "    do(broom::tidy(t.test(.$value, .$noise, paired=TRUE))) ->\n",
    "    tvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another optional intermediate save:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# readr::write_csv(tvals, \"t-values.csv\")\n",
    "# readr::read_csv(\"t-values.csv\") -> tvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we merge the t-values back into the main dataframe, so we can use them to filter the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge t-values back into main dataframe\n",
    "all_data %>%\n",
    "    left_join(tvals, by=c(\"hemi\", \"vertex\")) ->\n",
    "    all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional save/load point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save(all_data, file=\"processed_data_with_tvals.RData\")\n",
    "# load(\"processed_data_with_tvals.RData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's filter on t-values greater than 4, and use that as our ROI to compare the cohorts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;246m# A tibble: 1 x 10\u001b[39m\n",
      "  estimate estimate1 estimate2 statistic  p.value parameter conf.low conf.high\n",
      "     \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m     \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m     \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m     \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m    \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m     \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m    \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m     \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m\n",
      "\u001b[38;5;250m1\u001b[39m     1.48      57.2      55.7      7.96 1.73\u001b[38;5;246me\u001b[39m\u001b[31m-15\u001b[39m   \u001b[4m1\u001b[24m\u001b[4m6\u001b[24m\u001b[4m6\u001b[24m173.     1.12      1.85\n",
      "\u001b[38;5;246m# … with 2 more variables: method \u001b[3m\u001b[38;5;246m<chr>\u001b[38;5;246m\u001b[23m, alternative \u001b[3m\u001b[38;5;246m<chr>\u001b[38;5;246m\u001b[23m\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "all_data %>%\n",
    "    filter(statistic >= 4, timepoint == \"pre\", freq == 2) %>%\n",
    "    do(broom::tidy(t.test(.$value ~ .$pretest))) ->\n",
    "    pretest_tvals\n",
    "\n",
    "print(pretest_tvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A p-value of `1.73e-15`; I'd say that's a significant effect!\n",
    "\n",
    "And now for the intervention effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in mean.default(y):\n",
      "“argument is not numeric or logical: returning NA”\n",
      "Warning message in var(y):\n",
      "“NAs introduced by coercion”\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in if (stderr < 10 * .Machine$double.eps * max(abs(mx), abs(my))) stop(\"data are essentially constant\"): missing value where TRUE/FALSE needed\n",
     "output_type": "error",
     "traceback": [
      "Error in if (stderr < 10 * .Machine$double.eps * max(abs(mx), abs(my))) stop(\"data are essentially constant\"): missing value where TRUE/FALSE needed\nTraceback:\n",
      "1. all_data %>% filter(statistic >= 4, freq == 2) %>% tidyr::pivot_wider(names_from = timepoint, \n .     values_from = value, id_cols = c(subject, freq, hemi, vertex, \n .         intervention)) %>% mutate(post_minus_pre = post - pre) %>% \n .     do(broom::tidy(t.test(.$post_minus_pre, .$intervention)))",
      "2. withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))",
      "3. eval(quote(`_fseq`(`_lhs`)), env, env)",
      "4. eval(quote(`_fseq`(`_lhs`)), env, env)",
      "5. `_fseq`(`_lhs`)",
      "6. freduce(value, `_function_list`)",
      "7. withVisible(function_list[[k]](value))",
      "8. function_list[[k]](value)",
      "9. do(., broom::tidy(t.test(.$post_minus_pre, .$intervention)))",
      "10. do.data.frame(., broom::tidy(t.test(.$post_minus_pre, .$intervention)))",
      "11. eval_tidy(args[[1]], mask)",
      "12. broom::tidy(t.test(.$post_minus_pre, .$intervention))",
      "13. t.test(.$post_minus_pre, .$intervention)",
      "14. t.test.default(.$post_minus_pre, .$intervention)"
     ]
    }
   ],
   "source": [
    "all_data %>%\n",
    "    filter(statistic >= 4, freq == 2) %>%\n",
    "    tidyr::pivot_wider(names_from=timepoint, values_from=value, id_cols=c(subject, freq, hemi, vertex, intervention)) %>%\n",
    "    mutate(post_minus_pre=post - pre) %>%\n",
    "    do(broom::tidy(t.test(.$post_minus_pre, .$intervention))) ->\n",
    "    intervention_tvals\n",
    "\n",
    "print(pretest_tvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\tWelch Two Sample t-test\n",
       "\n",
       "data:  post_minus_pre by intervention\n",
       "t = -64.049, df = 169973, p-value < 2.2e-16\n",
       "alternative hypothesis: true difference in means is not equal to 0\n",
       "95 percent confidence interval:\n",
       " -14.94884 -14.06110\n",
       "sample estimates:\n",
       "mean in group language   mean in group letter \n",
       "             -9.150898               5.354074 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_data %>%\n",
    "    filter(statistic >= 4, freq == 2) %>%\n",
    "    tidyr::pivot_wider(names_from=timepoint, values_from=value, id_cols=c(subject, freq, hemi, vertex, intervention)) %>%\n",
    "    mutate(post_minus_pre=post - pre) %>%\n",
    "    t.test(post_minus_pre ~ intervention, data=.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, a highly significant result, it seems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
