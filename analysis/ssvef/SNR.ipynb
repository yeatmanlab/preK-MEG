{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the SSVEP data\n",
    "\n",
    "Let's load a couple libraries we'll use repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(dplyr, warn.conflicts=FALSE)\n",
    "library(ggplot2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe we're about to load has already undergone fourier transform & projection to FSAverage cortical space, so we're dealing with vertex numbers and frequency bin amplitudes, across subject and pre/post intervention measurement times.  This assumes it's in the same folder as the notebook; adjust path as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc.time() -> start\n",
    "readr::read_csv(\"all_subjects-fsaverage-freq_domain-stc.csv\") -> all_data\n",
    "print(proc.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add in some metadata about who is in which group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"https://raw.githubusercontent.com/yeatmanlab/SSWEF/master/params\" -> param_dir\n",
    "file.path(\"..\", \"..\", \"params\") -> param_dir\n",
    "\n",
    "# load intervention groups\n",
    "file.path(param_dir, \"intervention_cohorts.yaml\") -> intervention_file\n",
    "yaml::read_yaml(intervention_file) -> intervention\n",
    "\n",
    "# load pre-test cohorts\n",
    "file.path(param_dir, \"letter_knowledge_cohorts.yaml\") -> pretest_file\n",
    "yaml::read_yaml(pretest_file) -> pretest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll define a function that will compute noise (the mean of the 2 frequency bins below and 2 frequency bins above the current bin). We'll apply it within a `group_by` operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute noise (mean of 2 bins above and below)\n",
    "compute_noise <- function(x) {\n",
    "    (lag(x, 2, 0) + lag(x, 1, 0) + lead(x, 1, 0) + lead(x, 2, 0)) / 4 -> noise\n",
    "    return(noise)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll merge in the metadata, separate the `source` column into separate hemisphere and vertex number columns, and compute `noise` and `snr` all in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes several minutes... might be worth it to split it into steps that assign back to `all_data`\n",
    "# the comment lines are natural points at which to do this.\n",
    "all_data %>%\n",
    "    # merge in metadata\n",
    "    mutate(subj_num=stringr::str_sub(subject, -4),\n",
    "           pretest=ifelse(subj_num %in% pretest$LowerKnowledge, 'lower', 'upper'),\n",
    "           intervention=ifelse(subj_num %in% intervention$LetterIntervention, 'letter', 'language')) %>%\n",
    "    # separate the hemispheres\n",
    "    tidyr::separate(source, c(\"hemi\", \"vertex\"), sep=\"_\") %>%\n",
    "    mutate(hemi=stringr::str_to_lower(hemi),\n",
    "           vertex=as.integer(vertex),\n",
    "           source=NULL) %>%\n",
    "    # compute noise and SNR\n",
    "    group_by(subject, timepoint, hemi, vertex) %>%\n",
    "    mutate(noise=compute_noise(value),\n",
    "           snr=value/noise) %>%\n",
    "    ungroup() ->\n",
    "    all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save ourselves the trouble of having to do that again, by saving the processed data, so we can skip ahead next time we run this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(all_data, \"processed_data.RData\")  # binary format, more efficient\n",
    "# readr::write_csv(all_data, \"processed_data.csv\")\n",
    "# readr::read_csv(\"processed_data.csv\") -> all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dataframe has all the variables we want, let's compute our (uncorrected) t-test of \"signal bin\" vs \"mean of adjacent noise bins\". This will yield a new dataframe with the t-test results. We'll write out the t-value table too, so we don't need to recompute it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute signal-vs-noise uncorrected t-test\n",
    "all_data %>%\n",
    "    filter(freq == 2, timepoint == \"pre\") %>%\n",
    "    group_by(hemi, vertex) %>%\n",
    "    do(broom::tidy(t.test(.$value, .$noise, paired=TRUE))) ->\n",
    "    tvals\n",
    "\n",
    "# readr::write_csv(tvals, \"t-values.csv\")\n",
    "# readr::read_csv(\"t-values.csv\") -> tvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we merge the t-values back into the main dataframe, so we can use them to filter the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge t-values back into main dataframe\n",
    "all_data %>%\n",
    "    left_join(tvals, by=c(\"hemi\", \"vertex\")) ->\n",
    "    all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's filter on t-values greater than 4, and use that as our ROI to compare the cohorts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data %>%\n",
    "    filter(statistic >= 4, timepoint == \"pre\", freq == 2) %>%\n",
    "    do(broom::tidy(t.test(.$value ~ .$pretest))) ->\n",
    "    pretest_tvals\n",
    "\n",
    "summary(pretest_tvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for the intervention effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data %>%\n",
    "    filter(statistic >= 4, freq == 2) %>%\n",
    "    tidyr::pivot_wider(names_from=timepoint, values_from=value) %>%\n",
    "    mutate(post_minus_pre=post - pre) %>%\n",
    "    do(broom::tidy(t.test(.$post_minus_pre ~ .$intervention))) ->\n",
    "    intervention_tvals\n",
    "\n",
    "summary(pretest_tvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}